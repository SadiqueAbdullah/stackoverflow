{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last two questions regarding what are related to relationships of variables with salary and job satisfaction - Each of these questions will involve not only building some sort of predictive model, but also finding and interpretting the influential components of whatever model we build.\n",
    "\n",
    "To get started let's read in the necessary libraries and take a look at some of our columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Respondent</th>\n",
       "      <th>Professional</th>\n",
       "      <th>ProgramHobby</th>\n",
       "      <th>Country</th>\n",
       "      <th>University</th>\n",
       "      <th>EmploymentStatus</th>\n",
       "      <th>FormalEducation</th>\n",
       "      <th>MajorUndergrad</th>\n",
       "      <th>HomeRemote</th>\n",
       "      <th>CompanySize</th>\n",
       "      <th>...</th>\n",
       "      <th>StackOverflowMakeMoney</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HighestEducationParents</th>\n",
       "      <th>Race</th>\n",
       "      <th>SurveyLong</th>\n",
       "      <th>QuestionsInteresting</th>\n",
       "      <th>QuestionsConfusing</th>\n",
       "      <th>InterestedAnswers</th>\n",
       "      <th>Salary</th>\n",
       "      <th>ExpectedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Student</td>\n",
       "      <td>Yes, both</td>\n",
       "      <td>United States</td>\n",
       "      <td>No</td>\n",
       "      <td>Not employed, and not looking for work</td>\n",
       "      <td>Secondary school</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Strongly disagree</td>\n",
       "      <td>Male</td>\n",
       "      <td>High school</td>\n",
       "      <td>White or of European descent</td>\n",
       "      <td>Strongly disagree</td>\n",
       "      <td>Strongly agree</td>\n",
       "      <td>Disagree</td>\n",
       "      <td>Strongly agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Student</td>\n",
       "      <td>Yes, both</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Yes, full-time</td>\n",
       "      <td>Employed part-time</td>\n",
       "      <td>Some college/university study without earning ...</td>\n",
       "      <td>Computer science or software engineering</td>\n",
       "      <td>More than half, but not all, the time</td>\n",
       "      <td>20 to 99 employees</td>\n",
       "      <td>...</td>\n",
       "      <td>Strongly disagree</td>\n",
       "      <td>Male</td>\n",
       "      <td>A master's degree</td>\n",
       "      <td>White or of European descent</td>\n",
       "      <td>Somewhat agree</td>\n",
       "      <td>Somewhat agree</td>\n",
       "      <td>Disagree</td>\n",
       "      <td>Strongly agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Professional developer</td>\n",
       "      <td>Yes, both</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>No</td>\n",
       "      <td>Employed full-time</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>Computer science or software engineering</td>\n",
       "      <td>Less than half the time, but at least one day ...</td>\n",
       "      <td>10,000 or more employees</td>\n",
       "      <td>...</td>\n",
       "      <td>Disagree</td>\n",
       "      <td>Male</td>\n",
       "      <td>A professional degree</td>\n",
       "      <td>White or of European descent</td>\n",
       "      <td>Somewhat agree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Disagree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>113750.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Professional non-developer who sometimes write...</td>\n",
       "      <td>Yes, both</td>\n",
       "      <td>United States</td>\n",
       "      <td>No</td>\n",
       "      <td>Employed full-time</td>\n",
       "      <td>Doctoral degree</td>\n",
       "      <td>A non-computer-focused engineering discipline</td>\n",
       "      <td>Less than half the time, but at least one day ...</td>\n",
       "      <td>10,000 or more employees</td>\n",
       "      <td>...</td>\n",
       "      <td>Disagree</td>\n",
       "      <td>Male</td>\n",
       "      <td>A doctoral degree</td>\n",
       "      <td>White or of European descent</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Somewhat agree</td>\n",
       "      <td>Strongly agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Professional developer</td>\n",
       "      <td>Yes, I program as a hobby</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>No</td>\n",
       "      <td>Employed full-time</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Computer science or software engineering</td>\n",
       "      <td>Never</td>\n",
       "      <td>10 to 19 employees</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Respondent                                       Professional  \\\n",
       "0           1                                            Student   \n",
       "1           2                                            Student   \n",
       "2           3                             Professional developer   \n",
       "3           4  Professional non-developer who sometimes write...   \n",
       "4           5                             Professional developer   \n",
       "\n",
       "                ProgramHobby         Country      University  \\\n",
       "0                  Yes, both   United States              No   \n",
       "1                  Yes, both  United Kingdom  Yes, full-time   \n",
       "2                  Yes, both  United Kingdom              No   \n",
       "3                  Yes, both   United States              No   \n",
       "4  Yes, I program as a hobby     Switzerland              No   \n",
       "\n",
       "                         EmploymentStatus  \\\n",
       "0  Not employed, and not looking for work   \n",
       "1                      Employed part-time   \n",
       "2                      Employed full-time   \n",
       "3                      Employed full-time   \n",
       "4                      Employed full-time   \n",
       "\n",
       "                                     FormalEducation  \\\n",
       "0                                   Secondary school   \n",
       "1  Some college/university study without earning ...   \n",
       "2                                  Bachelor's degree   \n",
       "3                                    Doctoral degree   \n",
       "4                                    Master's degree   \n",
       "\n",
       "                                  MajorUndergrad  \\\n",
       "0                                            NaN   \n",
       "1       Computer science or software engineering   \n",
       "2       Computer science or software engineering   \n",
       "3  A non-computer-focused engineering discipline   \n",
       "4       Computer science or software engineering   \n",
       "\n",
       "                                          HomeRemote  \\\n",
       "0                                                NaN   \n",
       "1              More than half, but not all, the time   \n",
       "2  Less than half the time, but at least one day ...   \n",
       "3  Less than half the time, but at least one day ...   \n",
       "4                                              Never   \n",
       "\n",
       "                CompanySize  ... StackOverflowMakeMoney Gender  \\\n",
       "0                       NaN  ...      Strongly disagree   Male   \n",
       "1        20 to 99 employees  ...      Strongly disagree   Male   \n",
       "2  10,000 or more employees  ...               Disagree   Male   \n",
       "3  10,000 or more employees  ...               Disagree   Male   \n",
       "4        10 to 19 employees  ...                    NaN    NaN   \n",
       "\n",
       "  HighestEducationParents                          Race         SurveyLong  \\\n",
       "0             High school  White or of European descent  Strongly disagree   \n",
       "1       A master's degree  White or of European descent     Somewhat agree   \n",
       "2   A professional degree  White or of European descent     Somewhat agree   \n",
       "3       A doctoral degree  White or of European descent              Agree   \n",
       "4                     NaN                           NaN                NaN   \n",
       "\n",
       "  QuestionsInteresting QuestionsConfusing InterestedAnswers    Salary  \\\n",
       "0       Strongly agree           Disagree    Strongly agree       NaN   \n",
       "1       Somewhat agree           Disagree    Strongly agree       NaN   \n",
       "2                Agree           Disagree             Agree  113750.0   \n",
       "3                Agree     Somewhat agree    Strongly agree       NaN   \n",
       "4                  NaN                NaN               NaN       NaN   \n",
       "\n",
       "   ExpectedSalary  \n",
       "0             NaN  \n",
       "1         37500.0  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "\n",
       "[5 rows x 154 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('../survey_results_public.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with just quantitative variables is actually pretty straightforward \n",
    "# for most supervised learning techniques (except - we note that any row with \n",
    "# a missing value in any of the columns we use will be dropped -\n",
    "# which might leave us with fewer predictions than we were hoping for).\n",
    "# Let's just start and iterate on our findings\n",
    "\n",
    "# Let's just fit something and go from there\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above are variables that python is treating as numeric variables, and therefore, we \n",
    "# could send them into our linear model blindly to predict the response\n",
    "# Let's take a quick look at our data first\n",
    "\n",
    "df.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see that none of our variables appear to greatly correlated with salary\n",
    "# and we can see that if someone was given an expected salary question, they either\n",
    "# never answered the salary question or they were not given the salary question\n",
    "\n",
    "\n",
    "# We an still go ahead and make predictions using these variables as a reminder of the \n",
    "# scikit learn way of fitting models.  The process is similar to quickly fit models of \n",
    "# all types - usually a four step process of - instantiate, fit, predict, score\n",
    "# In most cases, we also will want to split data into training and test data to assure \n",
    "# we are not building models that overfit the data and do not extend well to new situations.\n",
    "\n",
    "X = df[['CareerSatisfaction', 'HoursPerWeek', 'JobSatisfaction', 'StackOverflowSatisfaction']]\n",
    "y = df['Salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Here you could set any hyperparameters of your model\n",
    "lm_model.fit(X_train, y_train) # If this model was to predict for new individuals, we probably would want\n",
    "               # worry about train/test splits and cross-validation, but for now I am most \n",
    "               # interested in finding a model that just fits all of the data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice the above breaks because of the NaN values, so we either need to fill or remove them\n",
    "# Or we could write a conditional model that fits differently \n",
    "# depending on the values that are missing - we can see the nans based on the describe above\n",
    "df.shape\n",
    "\n",
    "\n",
    "#________ Video 1 through here on introduction to the data - could do a bit more EDA ________#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The easiest way to move onto a conclusion in a first pass is probably just with dropping\n",
    "\n",
    "num_vars = df[['Salary', 'CareerSatisfaction', 'HoursPerWeek', 'JobSatisfaction', 'StackOverflowSatisfaction']]\n",
    "df_dropna = num_vars.dropna(axis=0)\n",
    "\n",
    "X = df_dropna[['CareerSatisfaction', 'HoursPerWeek', 'JobSatisfaction', 'StackOverflowSatisfaction']]\n",
    "y = df_dropna['Salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Here you could set any hyperparameters of your model\n",
    "lm_model.fit(X_train, y_train) # If this model was to predict for new individuals, we probably would want\n",
    "               # worry about train/test splits and cross-validation, but for now I am most \n",
    "               # interested in finding a model that just fits all of the data well\n",
    "\n",
    "        \n",
    "y_test_preds = lm_model.predict(X_test) #We can then use our fitted model to predict the salary for each\n",
    "                                        #indvidual in our test set, and see how well these predictions\n",
    "                                        #match the truth.\n",
    "\n",
    "print(r2_score(y_test, y_test_preds)) #In this case we are predicting a continuous, numeric response.  Therefore, common\n",
    "print(mean_squared_error(y_test, y_test_preds)) #metrics to assess fit include Rsquared and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whoop - we built a model that predicts... but we are missing by ALOT!\n",
    "# We can get a quick glimpse of how bad our predictions are...\n",
    "# This suggests that 3% of the variability in salaries can be explained by these variables...\n",
    "df_dropna.shape # But it also reduced our dataset to only 5338 rows \n",
    "                # ~20% of the original dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recorded from here up\n",
    "\n",
    "\n",
    "# Screencasts Remaining:\n",
    "1. Imputation - first results\n",
    "2. Categorical Variables - improved results, but what is happening?\n",
    "3. Combat Overfitting - one method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_vs_act = pd.DataFrame(np.hstack([y_test.values.reshape(y_test.size,1), y_test_preds.reshape(y_test.size,1)]))\n",
    "preds_vs_act.columns = ['actual', 'preds']\n",
    "preds_vs_act['diff'] = preds_vs_act['actual'] - preds_vs_act['preds']\n",
    "preds_vs_act.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can plot how far our predictions are from the actual values compaired to the\n",
    "### predicted values - you can see that it isn't uncommon for us to miss salaries by\n",
    "### 150000 and the overpredictions tend to be much worse than the underpredictions\n",
    "### THere also appears to be a trend where our differences decrease as the predicted\n",
    "### values increase on the test data.\n",
    "\n",
    "plt.plot(preds_vs_act['preds'], preds_vs_act['diff'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#______Video 2 ____Our First Modeling Attempt (Mark all the bad things)________#\n",
    "\n",
    "\n",
    "\n",
    "### There are tons of downfalls already - our predictions are pretty poor, we have predictions\n",
    "### for only 20% of the total values that actually hold salaries, and we are only using \n",
    "### quantitative variables to predict.\n",
    "\n",
    "### Given how bad the predictions are, we might not hurt anything by just filling the missing \n",
    "### values to make more predictions.\n",
    "\n",
    "#Here we fill on the column means\n",
    "df_fillna = num_vars.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "\n",
    "X = df_fillna[['CareerSatisfaction', 'HoursPerWeek', 'JobSatisfaction', 'StackOverflowSatisfaction']]\n",
    "y = df_fillna['Salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Here you could set any hyperparameters of your model\n",
    "lm_model.fit(X_train, y_train) # If this model was to predict for new individuals, we probably would want\n",
    "               # worry about train/test splits and cross-validation, but for now I am most \n",
    "               # interested in finding a model that just fits all of the data well\n",
    "\n",
    "        \n",
    "y_test_preds = lm_model.predict(X_test) #We can then use our fitted model to predict the salary for each\n",
    "                                        #indvidual in our test set, and see how well these predictions\n",
    "                                        #match the truth.\n",
    "\n",
    "print(r2_score(y_test, y_test_preds)) #In this case we are predicting a continuous, numeric response.  Therefore, common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we can predict on everything, but our predictions are even worse!\n",
    "\n",
    "preds_vs_act = pd.DataFrame(np.hstack([y_test.values.reshape(y_test.size,1), y_test_preds.reshape(y_test.size,1)]))\n",
    "preds_vs_act.columns = ['actual', 'preds']\n",
    "preds_vs_act['diff'] = preds_vs_act['actual'] - preds_vs_act['preds']\n",
    "preds_vs_act.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds_vs_act['preds'], preds_vs_act['diff'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds_vs_act['preds'], preds_vs_act['actual'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('actual'); #This looks less compelling that we are predicting well...\n",
    "# I also think I found the mean amount...which aren't real 'actual' salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some strange line here - probably because we filled in our average for everything\n",
    "### Which was actually data leakage.  We shouldn't have done this at all. We would likely\n",
    "### Have to use the mean of the old data to fill in the missing of the future data...\n",
    "\n",
    "### But this does depend a bit - if on future homes, you will have the x-variables before\n",
    "### having to predict, this really isn't data leakage, as you would have the abiltiy to update\n",
    "### the inputed means with each new individual in your dataset.\n",
    "\n",
    "### Really the values that have the mean value for the salary should be dropped - because\n",
    "### those are not true salaries.\n",
    "\n",
    "df_fillna = df_fillna.drop(df_fillna[df_fillna['Salary'] == np.mean(df['Salary'])].index)\n",
    "df_fillna.shape # that's better. we only have this many non-null salaries in our original dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below you can fit a new model with the missing salaries removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_fillna[['CareerSatisfaction', 'HoursPerWeek', 'JobSatisfaction', 'StackOverflowSatisfaction']]\n",
    "y = df_fillna['Salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Here you could set any hyperparameters of your model\n",
    "lm_model.fit(X_train, y_train) # If this model was to predict for new individuals, we probably would want\n",
    "               # worry about train/test splits and cross-validation, but for now I am most \n",
    "               # interested in finding a model that just fits all of the data well\n",
    "\n",
    "        \n",
    "y_test_preds = lm_model.predict(X_test) #We can then use our fitted model to predict the salary for each\n",
    "                                        #indvidual in our test set, and see how well these predictions\n",
    "                                        #match the truth.\n",
    "\n",
    "print(r2_score(y_test, y_test_preds)) #In this case we are predicting a continuous, numeric response.  Therefore, common\n",
    "print(mean_squared_error(y_test, y_test_preds)) #metrics to assess fit include Rsquared and MSE.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Stop Video 2\n",
    "\n",
    "\n",
    "\n",
    "### Now we can predict on everything, but our predictions are even worse!\n",
    "\n",
    "preds_vs_act = pd.DataFrame(np.hstack([y_test.values.reshape(y_test.size,1), y_test_preds.reshape(y_test.size,1)]))\n",
    "preds_vs_act.columns = ['actual', 'preds']\n",
    "preds_vs_act['diff'] = preds_vs_act['actual'] - preds_vs_act['preds']\n",
    "preds_vs_act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds_vs_act['preds'], preds_vs_act['diff'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### When we see fan like shapes in the residual plots like this - it often suggests\n",
    "### we might make better predictions on the log of the response\n",
    "\n",
    "plt.plot(preds_vs_act['preds'], preds_vs_act['actual'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('actual'); #there appears to be a slight positive trend like we would want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#______Video 3 Fill in Missing values with the mean - why this is bad_______#\n",
    "\n",
    "\n",
    "### Let's see how we might be able to use categorical variables in our models.\n",
    "### Though you might try to do something smart to reduce the feature space of your\n",
    "### x-matrix (like find curved relationships that exist in salary comparing across categories).\n",
    "### It is probably easier to just blindly encode all of the categorical variables as dummy\n",
    "### variables in our models.\n",
    "\n",
    "cat_vars_int = df.select_dtypes(include=['object']).copy().columns\n",
    "# http://pbpython.com/categorical-encoding.html\n",
    "\n",
    "len(cat_vars_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now that we have a list of all the dummy variables we might be interested in... \n",
    "### Let's dummy code them, so that we can use them in our machine learning models\n",
    "### you can do this with pandas (get dummies) or with sklearn (one hot encoding)\n",
    "### Feel free to use whatever you are comfortable with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in  cat_vars_int:\n",
    "    # for each cat add dummy var, drop original column\n",
    "    df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Because we have more rows than number of variables, it is actually possible\n",
    "### for us to build a model that uses all of the columns to predict the response...\n",
    "### Whether this is actually a good idea or not is up for debate - let's maybe\n",
    "### choose some variables that seem like they might be related to salary and go from there.\n",
    "\n",
    "### You can also see that the nulls are still dropped after dummy encoding, which means\n",
    "### we will again need to figure out what to do with rows where those values are null.\n",
    "### It might be okay to just use the mode of the dataset to fill in those values - though\n",
    "### in reality, a lack of answer is maybe an indication that your answer is different \n",
    "### from the group and therefore, you didn't want to answer the question.\n",
    "\n",
    "### We know there are 12891 non-NaN salaries to predict based on the previous model - so we\n",
    "### want to make sure we can predict all of these salaries with our new model as well, but now\n",
    "### unlike the 5 columns we had to choose from before we have more than 40,000 to choose from.\n",
    "### This could be a great place for some PCA or PLS, but I would like to try and keep \n",
    "### the interpretability of the features as much as possible... so I am just going to\n",
    "### use the original features. \n",
    "\n",
    "### We could try even adding interactions or other combinations of these features, but again\n",
    "### this would make our features less interpretable. So you have to weigh the pros and cons\n",
    "### of adding these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.concat([df, df_fillna], axis=1, join='inner')\n",
    "df_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['Salary'].head()['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result.iloc[:,~df_result.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we have no duplicated columns, we can focus on which of our new columns (and the \n",
    "### previously used columns) we would like to use to try and predict the response.  We might\n",
    "### just go based on intuition, or we could try to find the variables that are most correlated\n",
    "### Don't get too high of hopes - having a quant variable correlated with a 1-0 variable\n",
    "### is not really what correlation coefficients are designed to detect.  They are meant\n",
    "### to find linear relationships between quant variables. Though correlations are not built for\n",
    "### finding these relations - they can still give a sense of which variables are best related\n",
    "\n",
    "\n",
    "### Actually if you try to build the correlation matrix... it might run for a long time, and\n",
    "### not be very legible anyway... Let's just fit some stuff that seems interesting \n",
    "### and intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Given how many columns we have to use - let's just drop all of the columns that have any\n",
    "### missing values\n",
    "\n",
    "df_result = df_result.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.shape # which is only 6, sooo that kind of sucks at narrowing down this mess..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_result['Salary']\n",
    "X = df_result.drop(['Respondent', 'Salary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n",
    "\n",
    "#lm_model = LinearRegression(normalize=True) # Here you could set any hyperparameters of your model\n",
    "#lm_model.fit(X_train, y_train) # If this model was to predict for new individuals, we probably would want\n",
    "               # worry about train/test splits and cross-validation, but for now I am most \n",
    "               # interested in finding a model that just fits all of the data well\n",
    "\n",
    "        \n",
    "#y_test_preds = lm_model.predict(X_test) #We can then use our fitted model to predict the salary for each\n",
    "                                        #indvidual in our test set, and see how well these predictions\n",
    "                                        #match the truth.\n",
    "\n",
    "#print(r2_score(y_test, y_test_preds)) #In this case we are predicting a continuous, numeric response.  Therefore, common\n",
    "#print(mean_squared_error(y_test, y_test_preds)) #metrics to assess fit include Rsquared and MSE.  \n",
    "## Filling in the missing values does appear to have helped based on a preliminary check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(r2_score(y_train, lm_model.predict(X_train)))\n",
    "#print(mean_squared_error(y_train, lm_model.predict(X_train))) # What does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat the overfitting we have a number of options, but one way that would also reduce our run time would be to remove columns from our dataframe.  You will notice that sklearn does not provide pvals back for our coefficients, but it performs ridge regression by default.  So, therefore, we can consider that columns that have larger coefficients are also more useful for predicting our response variable.  How large is large enough to consider keeping? Well, that is a great question, and I also don't have a great answer...  We can try some stuff and see what works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can also run cross-validation and aggregate our results to combat the overfitting we saw earlier using this reduced X matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could deal with these rare events in different ways - you could consider them as great predictors\n",
    "# I am going to remove them - as I feel like they are likely not that indicative of other individuals\n",
    "# I want to find overriding truths about the individuals who receive particular salaries.\n",
    "# So, let's only consider columns where there are more than 1000 of the level of interest in the column.\n",
    "\n",
    "reduce_X = X.iloc[:, np.where((X.sum() > 10) == True)[0]]\n",
    "reduce_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = .30, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Here you could set any hyperparameters of your model\n",
    "lm_model.fit(X_train, y_train) # If this model was to predict for new individuals, we probably would want\n",
    "               # worry about train/test splits and cross-validation, but for now I am most \n",
    "               # interested in finding a model that just fits all of the data well\n",
    "\n",
    "        \n",
    "y_test_preds = lm_model.predict(X_test) #We can then use our fitted model to predict the salary for each\n",
    "                                        #indvidual in our test set, and see how well these predictions\n",
    "                                        #match the truth.\n",
    "\n",
    "print(r2_score(y_test, y_test_preds)) #In this case we are predicting a continuous, numeric response.  Therefore, common\n",
    "print(mean_squared_error(y_test, y_test_preds)) #metrics to assess fit include Rsquared and MSE.  \n",
    "## Filling in the missing values does appear to have helped based on a preliminary check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_train, lm_model.predict(X_train))) #In this case we are predicting a continuous, numeric response.  Therefore, common\n",
    "print(mean_squared_error(y_train, lm_model.predict(X_train))) #metrics to assess fit include Rsquared and MSE.  \n",
    "## Filling in the missing values does appear to have helped based on a preliminary check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's see what be the best number of features to use based on the test set performance\n",
    "def find_optimal_lm_mod(X, y, cutoffs, test_size = .30, random_state=42, plot=True):\n",
    "    '''\n",
    "    INPUT\n",
    "    X - pandas dataframe, X matrix\n",
    "    y - pandas dataframe, response variable\n",
    "    cutoffs - list of ints, cutoff for number of non-zero values in dummy categorical vars\n",
    "    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n",
    "    random_state - int, default 42, controls random state for train_test_split\n",
    "    plot - boolean, default 0.3, True to plot result\n",
    "    \n",
    "    OUTPUT\n",
    "    r2_scores_test - list of floats of r2 scores on the test data\n",
    "    r2_scores_train - list of floats of r2 scores on the train data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n",
    "    for cutoff in cutoffs:\n",
    "        \n",
    "        #reduce X matrix\n",
    "        reduce_X = X.iloc[:, np.where((X.sum() > cutoff) == True)[0]]\n",
    "        num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "        #split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "        #fit the model and obtain pred response\n",
    "        lm_model = LinearRegression(normalize=True) \n",
    "        lm_model.fit(X_train, y_train)\n",
    "        y_test_preds = lm_model.predict(X_test)\n",
    "        y_train_preds = lm_model.predict(X_train)\n",
    "        \n",
    "        #append the r2 value from the test set\n",
    "        r2_scores_test.append(r2_score(y_test, y_test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, y_train_preds))\n",
    "        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5)\n",
    "        plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Rsquared')\n",
    "        plt.title('Rsquared by Number of Features')\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "        \n",
    "    best_cutoff = max(results, key=results.get)\n",
    "    \n",
    "    #reduce X matrix\n",
    "    reduce_X = X.iloc[:, np.where((X.sum() > int(best_cutoff)) == True)[0]]\n",
    "    num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "    #split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "    #fit the model\n",
    "    lm_model = LinearRegression(normalize=True) \n",
    "    lm_model.fit(X_train, y_train)\n",
    "        \n",
    "    return r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [5000, 3500, 2500, 1000, 100, 50, 30, 20, 10, 5]\n",
    "r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = find_optimal_lm_mod(X, y, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#______Video 4 Creating Dummy Variables & Other Alternatives for Categorical Variables____#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Now that we have the best model in terms of the r2 on the test data, we can use this model to see which features\n",
    "### appear to be most important, and what impact they have on salary.\n",
    "\n",
    "X_train.shape # we have 1081 features in the optimal model - let's look at some of them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = lm_model.predict(X_test)\n",
    "\n",
    "preds_vs_act = pd.DataFrame(np.hstack([y_test.values.reshape(y_test.size,1), y_test_preds.reshape(y_test.size,1)]))\n",
    "preds_vs_act.columns = ['actual', 'preds']\n",
    "preds_vs_act['diff'] = preds_vs_act['actual'] - preds_vs_act['preds']\n",
    "\n",
    "plt.plot(preds_vs_act['preds'], preds_vs_act['diff'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds_vs_act['preds'], preds_vs_act['actual'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('actual'); #there appears to be a slight positive trend like we would want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df = pd.DataFrame()\n",
    "\n",
    "coefs_df['est_int'] = X_train.columns\n",
    "coefs_df['coefs'] = lm_model.coef_\n",
    "coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n",
    "\n",
    "coefs_df.sort_values('abs_coefs', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, sum(X_train['Professional_Professional developer'])\n",
    "\n",
    "\n",
    "#_____Video 7 Interpretting the results_____#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____Video 8 - Ensemble Models______#\n",
    "\n",
    "### One of the best out of the box methods for supervised machine learning\n",
    "### is known as the RandomForest - let's see if we can use this model to outperform\n",
    "### The linear model from earlier.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "### Let's see what be the best number of features to use based on the test set performance\n",
    "def find_optimal_rf_mod(X, y, cutoffs, test_size = .30, random_state=42, plot=True):\n",
    "    '''\n",
    "    INPUT\n",
    "    X - pandas dataframe, X matrix\n",
    "    y - pandas dataframe, response variable\n",
    "    cutoffs - list of ints, cutoff for number of non-zero values in dummy categorical vars\n",
    "    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n",
    "    random_state - int, default 42, controls random state for train_test_split\n",
    "    plot - boolean, default 0.3, True to plot result\n",
    "    kwargs - include the arguments you want to pass to the rf model\n",
    "    \n",
    "    OUTPUT\n",
    "    r2_scores_test - list of floats of r2 scores on the test data\n",
    "    r2_scores_train - list of floats of r2 scores on the train data\n",
    "    rf_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n",
    "    for cutoff in cutoffs:\n",
    "        \n",
    "        #reduce X matrix\n",
    "        reduce_X = X.iloc[:, np.where((X.sum() > cutoff) == True)[0]]\n",
    "        num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "        #split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "        #fit the model and obtain pred response\n",
    "\n",
    "        rf_model = RandomForestRegressor()  #no normalizing here, but could tune other hyperparameters\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        y_test_preds = rf_model.predict(X_test)\n",
    "        y_train_preds = rf_model.predict(X_train)\n",
    "        \n",
    "        #append the r2 value from the test set\n",
    "        r2_scores_test.append(r2_score(y_test, y_test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, y_train_preds))\n",
    "        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5)\n",
    "        plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Rsquared')\n",
    "        plt.title('Rsquared by Number of Features')\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "        \n",
    "    best_cutoff = max(results, key=results.get)\n",
    "    \n",
    "    #reduce X matrix\n",
    "    reduce_X = X.iloc[:, np.where((X.sum() > int(best_cutoff)) == True)[0]]\n",
    "    num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "    #split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "    #fit the model\n",
    "    rf_model = RandomForestRegressor() \n",
    "    rf_model.fit(X_train, y_train)\n",
    "        \n",
    "    return r2_scores_test, r2_scores_train, rf_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [5000, 3500, 2500, 1000, 100, 50, 30, 20, 10, 5]\n",
    "r2_test, r2_train, rf_model, X_train, X_test, y_train, y_test = find_optimal_rf_mod(X, y, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = rf_model.predict(X_test)\n",
    "\n",
    "preds_vs_act = pd.DataFrame(np.hstack([y_test.values.reshape(y_test.size,1), y_test_preds.reshape(y_test.size,1)]))\n",
    "preds_vs_act.columns = ['actual', 'preds']\n",
    "preds_vs_act['diff'] = preds_vs_act['actual'] - preds_vs_act['preds']\n",
    "\n",
    "plt.plot(preds_vs_act['preds'], preds_vs_act['diff'], 'bo');\n",
    "plt.xlabel('predicted');\n",
    "plt.ylabel('difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like this overfits quite a bit... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### Let's see what be the best number of features to use based on the test set performance\n",
    "def find_optimal_rf_mod(X, y, cutoffs, test_size = .30, random_state=42, plot=True, param_grid=None):\n",
    "    '''\n",
    "    INPUT\n",
    "    X - pandas dataframe, X matrix\n",
    "    y - pandas dataframe, response variable\n",
    "    cutoffs - list of ints, cutoff for number of non-zero values in dummy categorical vars\n",
    "    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n",
    "    random_state - int, default 42, controls random state for train_test_split\n",
    "    plot - boolean, default 0.3, True to plot result\n",
    "    kwargs - include the arguments you want to pass to the rf model\n",
    "    \n",
    "    OUTPUT\n",
    "    r2_scores_test - list of floats of r2 scores on the test data\n",
    "    r2_scores_train - list of floats of r2 scores on the train data\n",
    "    rf_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "\n",
    "    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n",
    "    for cutoff in cutoffs:\n",
    "\n",
    "        #reduce X matrix\n",
    "        reduce_X = X.iloc[:, np.where((X.sum() > cutoff) == True)[0]]\n",
    "        num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "        #split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "        #fit the model and obtain pred response\n",
    "        if param_grid==None:\n",
    "            rf_model = RandomForestRegressor()  #no normalizing here, but could tune other hyperparameters\n",
    "\n",
    "        else:\n",
    "            rf_inst = RandomForestRegressor(n_jobs=-1, verbose=1)\n",
    "            rf_model = GridSearchCV(rf_inst, param_grid, n_jobs=-1) \n",
    "            \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        y_test_preds = rf_model.predict(X_test)\n",
    "        y_train_preds = rf_model.predict(X_train)\n",
    "\n",
    "        #append the r2 value from the test set\n",
    "        r2_scores_test.append(r2_score(y_test, y_test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, y_train_preds))\n",
    "        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5)\n",
    "        plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Rsquared')\n",
    "        plt.title('Rsquared by Number of Features')\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "        \n",
    "    best_cutoff = max(results, key=results.get)\n",
    "\n",
    "    #reduce X matrix\n",
    "    reduce_X = X.iloc[:, np.where((X.sum() > int(best_cutoff)) == True)[0]]\n",
    "    num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "    #split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "    #fit the model\n",
    "    if param_grid==None:\n",
    "        rf_model = RandomForestRegressor()  #no normalizing here, but could tune other hyperparameters\n",
    "\n",
    "    else:\n",
    "        rf_inst = RandomForestRegressor(n_jobs=-1, verbose=1)\n",
    "        rf_model = GridSearchCV(rf_inst, param_grid, n_jobs=-1) \n",
    "    rf_model.fit(X_train, y_train)\n",
    "     \n",
    "    return r2_scores_test, r2_scores_train, rf_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [5000, 3500, 2500, 1000, 100, 50, 30, 20, 10, 5]\n",
    "params = {'n_estimators': [10, 100, 1000], 'max_depth': [1, 5, 10, 100]}\n",
    "r2_test, r2_train, rf_model, X_train, X_test, y_train, y_test = find_optimal_rf_mod(X, y, cutoffs, param_grid=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
